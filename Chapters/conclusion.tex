\chapter{Conclusions and perspectives}
%\startcontents[chapters]
%\addcontentsline{toc}{chapter}{Conclusions and perspectives}  

\section{Conclusions}

This dissertation explored the abstraction capabilities of Transformer language models for syntactic processing. We sought to determine if these models rely mainly on surface-level patterns from their training data, or if they also implicitly construct abstract syntactic rules. Our research had two main objectives: first, to assess the potential of the autoregressive Transformer model as an explanatory tool for human syntactic processing; and second, to enhance interpretability methods for Transformer-based models. 

Our research makes two main contributions. First, we have introduced an integrated framework for assessing the linguistic capacities of Transformer-based models. Second, we applied this framework to evaluate the models on two aspects of syntactic abstraction: the capacity to represent hierarchical structures and the capacity to compositionally generalize observed structures. These evaluations conducted align closely with the key prerequisites specified in Section~\ref{sec:research_Q}, which are essential for a computational model to serve as a credible explanatory tool for human language processing. Our findings reveal that Transformers manage to represent hierarchical structures for nuanced syntactic generalization. However, instead of relying on systematic compositional rules, they seem to lean more towards lexico-categorical abstraction and structural similarity-based analogies~(\S\ref{chp:slog}).  
This study both highlights the potential of autoregressive Transformer models as explanatory tools for human syntactic processing and provides a methodological framework for their analysis and interpretability.


From a methodological standpoint, we introduce a comprehensive linguistically-informed analysis framework that builds upon and enhances recent interpretability techniques. The framework operates on three interrelated levels. First, behavioral assessment, grounded in challenge sets that target specific syntactic phenomena, serves as the foundational layer. This level assesses whether the model meets the requirement of reflecting human grammatical behavior. Although it reveals how the model behaves in response to certain inputs, it provides limited insight into its internal representations. Addressing this, the next level uses probing classifiers to locate the distribution of relevant syntactic information within the models. With these patterns identified, we introduce causal interventions as the third layer to decipher the underlying mechanisms driving a model's behavior and to evaluate their alignment with established linguistic analyses. This sets the stage for eventually modulating the model's behavior by tweaking the core components. In essence, our methodological framework serves two primary functions: it transforms linguistic theories into actionable, testable hypotheses, and enhances our ability to interpret and even guide Transformer-based models. In doing so, this framework takes a step toward fulfilling the `interpretability and controllability' criteria, essential for using Transformers to explain human language processing (\S~\ref{sec:research_Q}).


Our findings were twofold. First, our results in Chapter~\ref{chp:main_project} indicate that the Transformer model acquires remarkably nuanced representations of sentence structure, as evidenced by its strong performance on both behavioral-level tasks and measures of representational adequacy. Specifically, we curated challenge sets for subject-verb agreement across relative clauses and object past-participle agreement, which differ fundamentally in linguistic analysis despite their surface similarity. We then assessed whether the Transformer forms distinct representations for resolving these two agreements. Our heuristic-based evaluation in Section~\ref{sec:heuristics} highlights the model's strong ability for nuanced, structure-dependent generalizations that go beyond mere surface heuristics. Further exploration using probing classifiers~(\S\ref{sec:probing_location}) shows that syntactic information is mainly linearly encoded across all token representations between the two agreeing elements within a sentence. Despite this similar agreement information distribution pattern for both types of agreement, causal interventions experiments (\S\ref{sec:causal}) indicate that the model's predictions rely on linguistically relevant cues. These cues exhibit distinct patterns for different agreement phenomena, consistent with theoretical expectations. This evidence suggests that the autoregressive Transformer LM aligns with the key prerequisites for behavioral similarity and representational adequacy, as outlined in Section~\ref{sec:research_Q}. Along with our interpretability framework, this makes the model a promising tool for studying human syntactic processing.


Our second set of findings in Chapter~\ref{chp:slog} presents a contrasting narrative. When subjected to the SLOG tasks, designed to assess compositional generalization, Transformer models encountered significant difficulties. SLOG involves a semantic parsing task (i.e., mapping linguistic expressions to meaning representations).
The test is designed to have a systematic shift between training and evaluation sets, ensuring that success in the latter demands a level of compositional generalization. In this phase, we evaluated various Transformer models as well as a symbolic neural parser. While all models excelled in the in-domain test set, Transformer models, even the recent pre-trained ones, struggle to generalize to sentences with longer dependency and deeper levels of recursion --- areas where the symbolic parser performs much better. This divergence from human-like generalization, which allows for the interpretation of unfamiliar frames by systematically recombining known structures, suggests that Transformer models may rely on different or possibly insufficient underlying mechanisms.



The contrasting conclusions from Chapter~\ref{chp:main_project}, which highlights the model's proficiency in approximating hierarchical structures, and Chapter~\ref{chp:slog}, which underscores its limited compositional generalization capacity, paint a nuanced picture of Transformer models' syntactic abstraction capabilities. This suggests that Transformer-based models primarily rely on lexico-categorical abstraction and structural similarity-based analogies for syntactic representation. While this enables them to generalize over unseen sentences with familiar structures, thus handling a sophisticated form of grammatical productivity, they struggle to handle novel linguistic structures that require inducing systematic compositional rules. These results corroborate previous findings with RNN~\citep{baroni2020linguistic} and offer further empirical evidence that NLMs can achieve a certain level of abstraction for grammatical productivity without being truly compositional. Overall, this study highlights both the promise and potential limitations of autoregressive Transformer models as explanatory tools for human syntactic processing, and provides a methodological framework for its analysis and interpretability.

From a linguistic and cognitive science perspective, our positive results regarding the Transformer model's ability to represent hierarchical structures challenge the theory of syntactic nativism, which emphasizes innate structural properties. Our research reveals that an autoregressive Transformer language model, when exposed to human-scale learning data and trained merely to predict subsequent words, can grasp the intricacies of hierarchical-sensitive phenomena. This implies that the complexity of human syntactic competence could potentially be derived from exposure and general-purpose learning alone, without relying on innate linguistic priors. In this context, the Transformer model can set plausible lower bounds on the learnability of such abstractions, and provide a comparative baseline for understanding human syntactic processing. 

These positive results, coupled with the limitations observed in models' compositional structural generalizations, indicate that Transformer models can achieve structure-dependent generalization without systematically following compositional rules. Instead, they seem to rely mainly on lexico-categorical abstraction and analogies based on structural similarity. This provides constructive hypotheses about the learning and implementation of linguistic structure. On the other hand, while symbolic rules and recursive structures have traditionally been viewed as fundamental to our understanding of human language processing, they might not be the sole mechanisms for effective language processing. In particular, natural languages host many productive linguistic phenomena that follow less compositional, more complex principles, such as linguistic idiosyncrasies~\citep{dankers-etal-2022-paradox}, irregular inflections, and semi-lexicalized syntactic constraints~\citep{goldberg2004english}. Moving forward, investigating how Transformer models handle these phenomena could shed light on potential alternative cognitive strategies that remain underexplored in both human cognition and machine learning, presenting novel perspectives on computational approaches for linguistic productivity beyond the conventional rule-based compositionality. 
 

From a deep learning perspective, our research highlights both the capabilities and limitations of data-driven neural models like Transformers. While excel in tasks where vast amounts of data guide them, they seem to struggle when faced with genuine structural and compositional challenges. This prompts the question: Can we enhance the compositional capabilities of these models to boost their learning efficiency without scarifying generality? Current research trends point in this direction, with a focus on harnessing the intrinsic nature of language to refine neural network architectures. Recent efforts, such as the study of ~\cite{smolensky2022neurocompositional} on neurocompositional computing, suggest that by merging Compositionality and Continuity principles, there is potential to bridge the gap between symbolic and neural paradigms, pushing neural language models towards more robust compositional generalization. The SLOG test we developed (\S\ref{chp:slog}) can be a valuable tool to measure progress and guide model development. Crucially, by aligning models with compositional principles, we move closer to mirroring human cognitive processes, which could enhance their role as tools for understanding human language processing.



\section{Future work}
This dissertation highlights the potential of autoregressive Transformer language models as explanatory tools for the theoretical study of language and human linguistic processes. We introduced a methodological framework that facilitates testing linguistic hypotheses and conducting comparative studies between model behavior and human cognition. The next logical progression is to employ the model as an explanatory tool for human syntactic processing.

\paragraph{Autoregressive Transformer model as an explanatory tool for language processing:}
One direction that we initially aimed to explore in my thesis was the comparative analysis between human judgment and neural model behaviors on the two target agreement phenomena. While we have touched upon this topic preliminarily, a detailed study has not yet been conducted due to time limitations. Psycholinguistic studies have shown that humans also make agreement errors, with plural attractors being particularly error-prone. This is traditionally attributed to the markedness of plurals, whose features are more salient than the unmarked singular form during human language processing~\citep{BOCK199145,eberhard2005making}.
Our evaluation of neural language models on number agreement tasks (\S\ref{sec:h_eval_protocol}) revealed that performance drops with increasing sentence complexity (quantified by the heuristic count). This leads to pertinent questions: Do the patterns of errors echo between models and humans? Are human judgments also influenced by surface-level heuristics? Moreover, as detailed in Section~\ref{subsec:nonce_exp}, while models exhibited the capacity to extrapolate syntactic generalization even in semantically implausible contexts, one wonders: To what extent are rules governing linguistic structures separate from those guiding linguistic meanings?

Additionally, there is an evident human tendency to produce more accurate agreement when dealing with singular controllers\footnote{Controllers correspond to ``cues'' in our dissertation.} \citep{villata2017intervention}. This asymmetry is often linked to an inherent human bias towards producing default singular forms \citep{greenberg1963some,corbett2000gender}. Notably, our observations in Section~\ref{sec:freq_effects} reveal a similar singularity-plurality asymmetry within neural language models, with potential roots in the frequency-based biases of target verbs. This observation triggers a set of compelling inquiries: How does this resonate with established biases in the human cognitive system?
How do humans navigate and extract generalities from their linguistic stimuli? And, importantly, can the autoregressive Transformer language model shed light on the origins of these behaviors, especially when we manipulate model training data --- like data quantity, sentence structure complexity, and verb frequencies?

Another promising avenue for future research is to correlate model predictions with behavioral data. Number agreement tasks (\S\ref{chp:NA_tasks}) demonstrate how the outputs of the autoregressive language model can be directly used through minimal-pair comparisons. Another approach in the literature involves using the surprisal metric, calculated as the log of the inverse of the conditional word probability~\citep{hale2001probabilistic,levy2008expectation}. Given that in psycholinguistics, a word's surprisal linearly affects the reading time of native speakers~\citep{goodkind2018predictive,hale2001probabilistic}, using surprisal as a linking function allows comparisons between model output and human reading behavior, facilitating the testing of linguistic and cognitive theories. This could, for example, offer insights into the parallels between the Transformer model's attention mechanism and human working memory during reading tasks. Recent integrative modeling approaches, such as the one by~\cite{schrimpf2021neural}, linked neuropsychological data, behavioral responses, and computational model predictions. This establishes connections between neural activations, human responses to linguistic stimuli, and model-based surprisal values. Such endeavors can enhance our understanding of human language processing and refine our computational models to align more closely with human cognition.


\paragraph{Model interpretability} 
To effectively leverage Transformer language models for explaining human language processing, there is a pressing need to further illuminate their inner workings. Our framework, rooted in linguistic analysis, employs challenge sets, probing, and causal intervention methodologies. Yet, many interpretability methods lie outside the scope of our current exploration in this dissertation. 

A notable direction for further exploration is the neural-level analysis techniques, as detailed in the survey by~\cite{sajjad-etal-2022-neuron}. These techniques shed light on how models organize, specialize, and redundantly store knowledge, aligning well with our objectives. For instance, \cite{bau2018identifying} and ~\cite{dai-etal-2022-knowledge} have demonstrated how understanding neurons can help control the output of a model. Furthermore, such a granular understanding can guide the optimization of model architectures, possibly minimizing the required parameters~\citep{voita-etal-2019-analyzing,sajjad2004poor,dalvi-etal-2020-analyzing}. Aligning these capabilities with our objective can reinforce the potential of Transformer models as explanatory tools for human linguistic behaviors.

In Chapter~\ref{chp:slog}, we highlighted the challenges faced by Transformer models in compositional structural generalization. As a future endeavor, we aim to understand what makes compositional generalization difficult for NLMs. Specifically, how do Transformer models combine token-based information into representations for larger linguistic structures? Additionally, exploring recent hybrid methodologies, which blend symbolic and neural network paradigms, appears promising, for instance, neurocompositional computing by~\cite{smolensky2022neurocompositional}.