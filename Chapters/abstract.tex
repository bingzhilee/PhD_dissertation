
\paragraph{Titre :} Étude des capacités abstractives de modèles de langue neuronaux

\paragraph{Résumé (court) :} Les théories linguistiques traditionnelles postulent que la compétence linguistique humaine est fondée sur des propriétés structurelles innées et des représentations symboliques. Cependant, les modèles de langue à base de Transformeurs excellent dans diverses tâches de \ac{TAL} sans intégrer explicitement de tels prérequis linguistiques. Leur succès empirique remet en question ces hypothèses linguistiques établies et soulève des interrogations sur les mécanismes sous-jacents des modèles. Cependant, leur opacité et complexité, liées à un grand nombre de paramètres, rendent difficile la compréhension de leur fonctionnement interne. Cette thèse vise à éclaircir si les Transformeurs se basent essentiellement sur la reconnaissance de motifs superficiels pour représenter des structures syntaxiques, ou s'ils sont capables d’abstraire implicitement des règles plus générales. Deux objectifs principaux guident cette recherche : i) évaluer le potentiel du modèle de langue Transformeur autoregressif comme outil explicatif du traitement syntaxique humain ; ii) améliorer l’interprétabilité du modèle. Nous abordons ces objectifs en examinant les abstractions syntaxiques des modèles Transformeur sur deux niveaux : leur capacité à modéliser des structures hiérarchiques, et leur capacité à généraliser compositionnellement des structures observées. Nous introduisons un cadre d'analyse intégré comprenant trois niveaux interdépendants : évaluation comportementale à travers des ensembles de test de défis, analyse représentationnelle à l'aide de sondes linguistiques, et analyse fonctionnelle par interventions causales. Nous évaluons d'abord le modèle sur des tests syntaxiques afin de déterminer sa capacité à reproduire le comportement linguistique humain.  Ensuite, nous utilisons des sondes linguistiques et des interventions causales pour mesurer l'adéquation des représentations internes du modèle avec les théories linguistiques établies. Nos résultats montrent que les Transformeurs parviennent à représenter des structures hiérarchiques pour une généralisation syntaxique nuancée. Cependant, au lieu de s'appuyer sur des règles compositionnelles systématiques, il semble qu'ils se basent davantage sur l'abstraction lexico-catégorielle et des analogies structurelles. Si cela leur permet de gérer une forme sophistiquée de productivité grammaticale pour des structures familières, ils rencontrent des difficultés avec des structures qui nécessitent une application systématique des règles compositionnelles. Cette étude met en évidence à la fois la promesse et les limitations potentielles des modèles Transformeur autoregressifs comme outils explicatifs pour le traitement syntaxique humain, et fournit un cadre méthodologique pour leur analyse et leur interprétabilité.


% Une question clé, qui est également le cœur de cette thèse, est de savoir si les Transformeurs construisent implicitement une forme de représentation hiérarchique abstraite. La complexité de ces modèles, avec leurs nombreux paramètres, rend difficile la compréhension de leur fonctionnement interne. 
%qui apprennent des représentations du langage à partir de textes non annotés,
% Alors que la recherche dans ce domaine est en croissance, l'étendue des capacités d'abstraction linguistique des Transformeurs reste une question ouverte. 
\paragraph{Mots-clés:} Traitement automatique des langues, modèles de langue neuronaux, interprétabilité, généralisation, abstraction linguistique,  representation syntaxique, structures hiérarchiques, compositionalité 



\newpage

\paragraph{Title:} Study of the abstraction capabilities of neural language models

\paragraph{Abstract:} Traditional linguistic theories have long posited that human language competence is founded on innate structural properties and symbolic representations. However, Transformer-based language models, which learn language representations from unannotated text, have excelled in various natural language processing (NLP) tasks without explicitly modeling such linguistic priors. Their empirical success challenges these long-standing linguistic assumptions and also raises questions about the models' underlying mechanisms for linguistic competence. However, the black-box nature and complexity of these models, due to their numerous parameters, make it difficult to understand their internal workings. While research in this area is growing, the extent of their linguistic abstraction capabilities remains an open question. This thesis seeks to determine whether Transformer models primarily rely on surface-level patterns for representing syntactic structures, or if they also implicitly capture more abstract rules. The study serves two main objectives: i) assessing the potential of an autoregressive Transformer language model as an explanatory tool for human syntactic processing; ii) enhancing the model's interpretability. To achieve these goals, we assess the syntactic abstractions in Transformer models on two levels: first, the ability to represent hierarchical structures, and second, the ability to compositionally generalize observed structures. We introduce an integrated linguistically-informed analysis framework that consists of three interrelated layers: behavioral assessment through challenge sets, representational probing using linguistic probes, and functional analysis through causal intervention. 
Our analysis starts with assessing the model's performance on syntactic challenge sets to see how closely it mirrors human language behavior. Following this, we use linguistic probes and causal interventions to assess how well the model's internal representations align with established linguistic theories. 
Our findings reveal that Transformers manage to represent hierarchical structures for nuanced syntactic generalization. However, instead of relying on systematic compositional rules, they seem to lean more towards lexico-categorical abstraction and structural analogies. While this allows them to handle a sophisticated form of grammatical productivity for familiar structures, they encounter challenges with structures that require a systematic application of compositional rules. This study highlights both the promise and potential limitations of autoregressive Transformer models as explanatory tools for human syntactic processing, and provides a methodological framework for its analysis and interpretability.
%

\paragraph{Keywords:} natural language processing, neural language models, linguistic abstraction, interpretability,  generalization, syntactic representation, hierarchical structures, compositionality




%\newpage
